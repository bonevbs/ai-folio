<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>An introduction to numerical optimization with Python (Part 1) | Boris Bonev</title> <meta name="author" content="Boris Bonev"> <meta name="description" content="A visual exploration of first and second-order optimization methods with PyTorch"> <meta name="keywords" content="boris bonev, ml-research, scientific-computing, research-scientist"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://bonevbs.github.io/posts/numerical-optimization/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Boris </span>Bonev</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">research</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">An introduction to numerical optimization with Python (Part 1)</h1> <p class="post-meta">December 17, 2021</p> <p class="post-tags"> <a href="/blog/2021"> <i class="fa-solid fa-calendar fa-sm"></i> 2021 </a>   ·   <a href="/blog/tag/numerical-optimization"> <i class="fa-solid fa-hashtag fa-sm"></i> Numerical optimization</a>   <a href="/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/tag/python"> <i class="fa-solid fa-hashtag fa-sm"></i> Python</a>   <a href="/blog/tag/pytorch"> <i class="fa-solid fa-hashtag fa-sm"></i> PyTorch</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>This is the first post in a series of posts that I am planning to write on the topic of machine learning. This article introduces fundamental algorithms in numerical optimization. For now, this is the Gradient Descent and Netwon algorithm. I might extend it with momentum based methods and conjugate gradient methods in the future. All of the posts are essentially Jupyter notebooks that I will publish in <a href="https://github.com/bonevbs/ml_notebooks" rel="external nofollow noopener" target="_blank">this repository</a>.</p> <h2 id="introduction">Introduction</h2> <p>Numerical optimization is an important tool in todays machine learning pipeline and optimization algorithms are often used as a black-box tool. Many problems boil down to the minimization (or maximization for that matter) of an objective function $f: \mathbb{R}^d \rightarrow \mathbb{R}$ with respect to the input $x \in \mathbb{R}^d$. It is important to understand these algorithms and under which conditions they perform well, so that they may be applied.</p> <p>We consider convex problems with sufficient regularity, such that a global minimum exists and so that gradient information may be used. Algorithms that use these properties can be broadly classified as either <strong>Line Search</strong> methods or <strong>Trust Region</strong> methods. The exposition here follows Nocedal and Wright, which is an excellent introduction to the topic. I am not aiming for mathematical rigor, nor for completeness. Rather, I would like to revise some of the key concepts myself and give a nice first introduction, complete with intuition and some working code.</p> <h3 id="preparation">Preparation</h3> <p>Before we explore some approaches, we need a differentiable function complete with gradient information. As we want make ourselves familiar with ML-relevant tools out there, we will make use of the library PyTorch and its auto-differentiation capabilities. As usual, numpy and matplotlib will be useful as well:</p> <p><strong>Disclaimer:</strong> the code in this article could be greatly simplified by just using numpy instead of PyTorch. If you are mainly interested in numerical optimization, I suggest to do this, as this will clarify the core concepts.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.autograd</span> <span class="k">as</span> <span class="n">autograd</span>
</code></pre></div></div> <p>Let us define a two-dimensional objective function, which we will be minimizing.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">d</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1"># random quadratic form
</span><span class="k">class</span> <span class="nc">quadratic_form</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">D_in</span><span class="p">):</span>
    <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">D_in</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># we do not use these, however I kept them for completeness, if we want a general quadratic form.
</span>    <span class="n">self</span><span class="p">.</span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">standard_exponential</span><span class="p">((</span><span class="n">D_in</span><span class="p">,</span><span class="n">D_in</span><span class="p">))</span>
    <span class="n">self</span><span class="p">.</span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">A</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">A</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">A</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">self</span><span class="p">.</span><span class="n">A</span> <span class="c1"># to ensure that A is a symmetric matrix
</span>  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">A</span><span class="nd">@x</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span> <span class="c1"># - 2.0 * self.b.T @ x + self.c
</span>    <span class="k">return</span> <span class="n">y</span>
</code></pre></div></div> <p>As we are using PyTorch, we can make use of auto-differentiation to evaluate the gradient information with respect to the input $x$. A nice introduction to autograd in PyTorch can be found <a href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html" rel="external nofollow noopener" target="_blank">here</a>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">objective_function</span> <span class="o">=</span> <span class="nf">quadratic_form</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>

<span class="c1"># getting the gradient
</span><span class="n">x0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">f0</span> <span class="o">=</span> <span class="nf">objective_function</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span> <span class="c1"># forward pass
</span><span class="n">f0</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span> <span class="c1"># after having computed one step, this will compute the gradient
</span><span class="nf">print</span><span class="p">(</span><span class="n">x0</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[0.5347],
        [1.0804]])
</code></pre></div></div> <p>Bingo! We will also need a function to visualize our minimizer:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_minimizier</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="n">trace</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">empty</span><span class="p">(</span><span class="mi">0</span><span class="p">)):</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="n">bounds</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">bounds</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">40</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="n">bounds</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">bounds</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="mi">40</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
  <span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">((</span><span class="n">X</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(),</span><span class="n">Y</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()))</span>
  <span class="n">Z</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">grid</span><span class="p">))</span>
  <span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">Z</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">(),</span> <span class="p">(</span><span class="mi">40</span><span class="p">,</span><span class="mi">40</span><span class="p">))</span>

  <span class="n">plt</span><span class="p">.</span><span class="nf">gca</span><span class="p">().</span><span class="nf">set_aspect</span><span class="p">(</span><span class="sh">"</span><span class="s">equal</span><span class="sh">"</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="nf">gcf</span><span class="p">().</span><span class="nf">set_size_inches</span><span class="p">(</span><span class="mf">7.5</span><span class="p">,</span> <span class="mf">5.5</span><span class="p">)</span>
  <span class="n">contourplot</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">contourf</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">levels</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">amin</span><span class="p">(</span><span class="n">Z</span><span class="p">),</span><span class="n">np</span><span class="p">.</span><span class="nf">amax</span><span class="p">(</span><span class="n">Z</span><span class="p">),</span><span class="mi">50</span><span class="p">))</span>
  <span class="n">cbar</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">colorbar</span><span class="p">(</span><span class="n">contourplot</span><span class="p">)</span>

  <span class="c1"># plot a sequence of points to visualize the optimization process
</span>  <span class="n">trace</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">trace</span><span class="p">.</span><span class="n">size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">trace</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">trace</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="sh">'</span><span class="s">w.</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">dashed</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">plot_minimizier</span><span class="p">(</span><span class="n">objective_function</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/images/posts/numerical_optimization_9_0.png" alt="png"></p> <p>We observe that the <code class="language-plaintext highlighter-rouge">objective_function</code> is a quadratic form with its minimum located at $(0, 0)^T$.</p> <h3 id="taylors-theorem">Taylor’s theorem</h3> <p>To illustrate both approaches, let us start with Taylor’s theorem. Suppose that $f: \mathbb{R}^d \rightarrow \mathbb{R}$ is continuously differentiable and that $p \in \mathbb{R}^d$. Then, we have that</p> \[f(x+p) = f(x) + \nabla f(x + tp)^T p,\] <p>for some $t \in (0,1)$. Furthermore, if $f$ is twice continuously differentiable,</p> \[\nabla f(x+p) = \nabla f(x) + \int_0^1 \nabla^2 f(x+tp) p\;\mathrm{d}t,\] <p>which implies</p> \[f(x+p) = f(x) + \nabla f(x)^T p + \frac{1}{2}p^T \nabla^2 f(x+tp) p,\] <p>for some $t \in (0,1)$. Here, we have introduced the vector-valued gradient $\nabla f$ and the $d \times d$ Hessian matrix $\nabla^2 f$.</p> <p>The latter implies $f(x+p) &lt; f(x)$ for $p = - \alpha \nabla f(x)$, if some sufficiently small $\alpha$ is chosen. We can therefore update $x$ sequentially, such that $f(x)$ keeps decreasing, thus converging to a minimum.</p> <h2 id="line-search-methods">Line Search methods</h2> <p>This idea brings us to line-search methods. The core idea of Line Search method is to update the solution $x_k$ iteratively according to</p> \[x_{k+1} = x_k + \alpha_k p_k,\] <p>where $p_k$ is a descent direction (which is not necessarily the negative gradient) and $\alpha_k$ a certain step size. Ideally, this will result in a decreasing sequence</p> \[f(x_0) \geq f(x_1) \geq \dots \geq f(x_{k-1}) \geq f(x_k)\] <p>which converges to a certain minimum. These approaches are also called line-search methods, as we are looking for the minimum across the line defined by the search direction $p_k$. It is evident that convergence will depend on the specific choice of the descent direction $p_k$ and step size $\alpha_k$.</p> <h3 id="gradient-descent">Gradient descent</h3> <p>The natural choice for $p_k$ is to take the direction of steepest descent $-\nabla f(x_k)$. Taking the appropriate step size is a more difficult problem to solve. This leaves the choice regarding the step size open. A popular strategy is the so-called backtracking line-search. At each iteration, the following algorithm will give us a step-size $\alpha$:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">backtrack</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">p0</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
  <span class="n">f0</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
  <span class="n">m</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">p0</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
  <span class="n">t</span> <span class="o">=</span> <span class="n">c</span> <span class="o">*</span> <span class="n">m</span>
  <span class="k">while</span> <span class="n">f0</span> <span class="o">-</span> <span class="nf">f</span><span class="p">(</span><span class="n">x0</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">p0</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">t</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">tau</span><span class="o">*</span><span class="n">alpha</span>
  <span class="k">return</span> <span class="n">alpha</span>

</code></pre></div></div> <p>In other words, we start out with a certain step size $\alpha$ and decrease it iteratively until the condition</p> \[f(x_0 + \alpha p_0) \leq f(x_0) + \alpha c ||p_0||^2,\] <p>is satisfied. $\tau \in (0,1)$ and $c \in (0,1)$ are some control parameters, which determine the behavior of the method. $\tau \in (0,1)$ is some shrinkage parameter, which controls the decay of $\alpha$, which is replaced by $\tau \alpha$ at each iteration. The parameter controls the maximal step size, as $\alpha$ will satisfy the above condition, which is also known as the Armijo–Goldstein condition (insert reference and say something about convergence).</p> <p>Now that we have a method for choosing a step size, we can proceed by iteratively updating</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">alpham</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>

  <span class="c1"># initialize the procedure
</span>  <span class="n">x</span> <span class="o">=</span> <span class="n">x0</span>
  <span class="n">x</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>
  <span class="n">f0</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">f0</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
  <span class="n">p</span> <span class="o">=</span> <span class="o">-</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span>
  <span class="n">alpha</span> <span class="o">=</span> <span class="n">alpham</span>

  <span class="c1"># track the progress in trace
</span>  <span class="n">trace</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()]</span>

  <span class="c1"># iterate 
</span>  <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">while</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="mi">100</span> <span class="ow">and</span> <span class="n">alpha</span> <span class="o">&gt;</span> <span class="n">tol</span><span class="p">:</span>
    <span class="c1"># for the following steps, we do not wish to track gradient information
</span>    <span class="c1"># to the end, we use torch.no_grad()
</span>    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
      <span class="n">alpha</span> <span class="o">=</span> <span class="nf">backtrack</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">alpham</span><span class="p">)</span>
      <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">p</span>
    <span class="n">trace</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">())</span>
    <span class="n">x</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="n">f</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="n">f0</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">f0</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">p</span> <span class="o">=</span> <span class="o">-</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span>
    <span class="n">k</span> <span class="o">+=</span> <span class="mi">1</span>
  <span class="c1"># compute the Wolfe condition
</span>  <span class="k">return</span> <span class="n">trace</span>

<span class="n">x0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([[</span><span class="mf">10.</span><span class="p">],[</span><span class="mf">10.</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">trace</span> <span class="o">=</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">objective_function</span><span class="p">,</span> <span class="n">x0</span><span class="p">)</span>
  
</code></pre></div></div> <p>We run the iteration 100 times or until the step size becomes too small. This gives us the following output, which is a list containing each of the iterates $x_k$:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">trace</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[array([[10.],
        [10.]], dtype=float32),
 array([[6.126855 ],
        [1.3573198]], dtype=float32),
 array([[ 4.9658213],
        [-1.0277541]], dtype=float32),
 array([[ 4.565513 ],
        [-1.6625257]], dtype=float32),
 array([[ 4.37983 ],
        [-1.808264]], dtype=float32)]
</code></pre></div></div> <p>Let us use the plotting routine from before to visualize the minimization process.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">trace</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
<span class="n">bounds</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">amin</span><span class="p">(</span><span class="n">trace</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span> <span class="o">-</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">amax</span><span class="p">(</span><span class="n">trace</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">amin</span><span class="p">(</span><span class="n">trace</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">amax</span><span class="p">(</span><span class="n">trace</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="mf">1.</span><span class="p">]</span>
<span class="nf">plot_minimizier</span><span class="p">(</span><span class="n">objective_function</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span> <span class="n">trace</span><span class="o">=</span><span class="n">trace</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/images/posts/numerical_optimization_19_0.png" alt="png"></p> <p>Very good! We can see that the updates are parallel to the gradient and move in the direction of steepest descent as we expected. Moreover, we observe that the step sizes $\alpha_k$ seem to be chosen reasonably, such that the method converges to the minimum. In fact, the Armillo-Goldstein condition with the chosen parameter is sufficient to guarantee convergence for a strictly convex function.</p> <p>Let us now try another example, the Rosenbrock function $f(x_1, x_2) = (1-x_1)^2 + 100 (x_2 - x_1^2)^2$, which is a pathological example, with a narrow, curved valley. This proves to be difficult for gradient descent methods. Let us define the function and apply gradient descent:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># We also need a pathological case
</span><span class="k">class</span> <span class="nc">rosenbrock_function</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
      <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
      <span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">100</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
      <span class="k">return</span> <span class="n">y</span>

<span class="n">objective_function</span> <span class="o">=</span> <span class="nf">rosenbrock_function</span><span class="p">()</span>

<span class="n">x0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">1.</span><span class="p">],[</span><span class="mf">1.</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">f0</span> <span class="o">=</span> <span class="nf">objective_function</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
<span class="n">trace</span> <span class="o">=</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">objective_function</span><span class="p">,</span> <span class="n">x0</span><span class="p">)</span>
</code></pre></div></div> <p>…and we visualize the result:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">trace</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
<span class="n">bounds</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">amin</span><span class="p">(</span><span class="n">trace</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span> <span class="o">-</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">amax</span><span class="p">(</span><span class="n">trace</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">amin</span><span class="p">(</span><span class="n">trace</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">amax</span><span class="p">(</span><span class="n">trace</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="mf">1.</span><span class="p">]</span>
<span class="nf">plot_minimizier</span><span class="p">(</span><span class="n">objective_function</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span> <span class="n">trace</span><span class="o">=</span><span class="n">trace</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/images/posts/numerical_optimization_23_0.png" alt="png"></p> <p>We observe that many iterations are spent zig-zagging in the curved part of the solution. This is understandable, considering that the gradient-descent method is moving in the “wrong” direction and therefore has to adjust the descent direction frequently.</p> <h2 id="trust-region-methods">Trust region methods</h2> <p>The previous problem nicely illustrated the need for a smarter choice of the descent direction $p_k$. An alternative approach can be derived upon closer inspection of Taylor’s theorem. If $f$ has sufficient regularity, the function $f$ can be approximated locally around $x$ with the quadratic form</p> \[f(x+p) = f(x) + \nabla f(x)^T p + \frac{1}{2} p^T \, \nabla^2 f(x) \, p + \mathcal{O}(||p||^3).\] <p>By differentiating this with respect to $p$ and setting the result to zero, we find $\hat{p}$, which minimizes this quadratic form. $\hat{p}$ satisfies</p> \[\nabla^2 f(x) \, \hat{p} = - \nabla f(x),\] <p>which can be solved to obtain $\hat{p} = - (\nabla^2 f(x))^{-1} \nabla f(x)$.</p> <h3 id="newton-method">Newton method</h3> <p>This gives rise to the Newton method. At each iteration, we compute the local quadratic approximation to $f$ and set $x_{k+1} = x_{k} + \hat{p}_k$ to be the minimizer of this local approximation. Coincidentally, this is equivalent tothe application of <em>Newton’s method for root-finding</em> to the Gradient $\nabla f(x)$, as we are essentially looking for a root of $\nabla f(x)$.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">newton_method</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x0</span><span class="p">):</span>

  <span class="c1"># initialize the procedure
</span>  <span class="n">x</span> <span class="o">=</span> <span class="n">x0</span>
  <span class="n">x</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>

  <span class="c1"># track the progress in trace
</span>  <span class="n">trace</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()]</span>

  <span class="c1"># iterate 
</span>  <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">while</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="mi">20</span><span class="p">:</span>
    <span class="c1"># compute the gradient and Hessian
</span>    <span class="n">H</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">hessian</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="nf">squeeze</span><span class="p">()</span>
    <span class="n">f0</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">f0</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="c1"># solve the linear system and compute the update
</span>    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
      <span class="n">p</span> <span class="o">=</span> <span class="o">-</span> <span class="n">torch</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">solve</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
      <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">p</span>
    
    <span class="n">trace</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">())</span>
    <span class="n">x</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="n">f</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="n">k</span> <span class="o">+=</span> <span class="mi">1</span>
  <span class="k">return</span> <span class="n">trace</span>

<span class="n">objective_function</span> <span class="o">=</span> <span class="nf">quadratic_form</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([[</span><span class="mf">10.</span><span class="p">],[</span><span class="mf">10.</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">trace</span> <span class="o">=</span> <span class="nf">newton_method</span><span class="p">(</span><span class="n">objective_function</span><span class="p">,</span> <span class="n">x0</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">trace</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
<span class="n">bounds</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">amin</span><span class="p">(</span><span class="n">trace</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span> <span class="o">-</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">amax</span><span class="p">(</span><span class="n">trace</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">amin</span><span class="p">(</span><span class="n">trace</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">amax</span><span class="p">(</span><span class="n">trace</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="mf">1.</span><span class="p">]</span>
<span class="nf">plot_minimizier</span><span class="p">(</span><span class="n">objective_function</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span> <span class="n">trace</span><span class="o">=</span><span class="n">trace</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/images/posts/numerical_optimization_27_0.png" alt="png"></p> <p>As we can see, the Newton method converged to the minimum in one step. This is hardly surprising as we have essentially approximated a quadratic form with a itself. As such, we find the global minimum in one step. Let us see how this method performs on the Rosenbrock function.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">objective_function</span> <span class="o">=</span> <span class="nf">rosenbrock_function</span><span class="p">()</span>

<span class="n">x0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">1.</span><span class="p">],[</span><span class="mf">1.</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">trace</span> <span class="o">=</span> <span class="nf">newton_method</span><span class="p">(</span><span class="n">objective_function</span><span class="p">,</span> <span class="n">x0</span><span class="p">)</span>

<span class="n">trace</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
<span class="n">bounds</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">amin</span><span class="p">(</span><span class="n">trace</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span> <span class="o">-</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">amax</span><span class="p">(</span><span class="n">trace</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">amin</span><span class="p">(</span><span class="n">trace</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">amax</span><span class="p">(</span><span class="n">trace</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="mf">1.</span><span class="p">]</span>
<span class="nf">plot_minimizier</span><span class="p">(</span><span class="n">objective_function</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span> <span class="n">trace</span><span class="o">=</span><span class="n">trace</span><span class="p">)</span>
</code></pre></div></div> <p><img src="/images/posts/numerical_optimization_29_0.png" alt="png"></p> <p>The Newton method converges to the desired solution in only two iterations, completely avoiding the previous problem. So, if the Newton method performs so well, why do we not always use it? The improved performance of the Newton method comes at the cost of evaluating the $d \times d$ Hessian matrix $\nabla^2 f(x)$ at each iteration and solving the linear system $\nabla^2 f(x) \, \hat{p} = - \nabla f(x)$. While this alone can be fairly prohibitive, there is also the question of regularity, as $f$ may not necessarily be smooth enough to have a Hessian everywhere. In other words, a local approximation with a quadratic form may not necessarily be a good approximation in such cases.</p> <p>Some algorithms try to alleviate the former problem of having to form the Hessian by only approximating it. This can be done in the gradient descent approach, by keeping track of the chenge in the descent direction and using this information to approximate the Hessian. The latter problem of insufficient regularity can be addressed by switching between line search approaches and trust region methods depending on the local smoothness of $f$.</p> <h2 id="conclusion">Conclusion</h2> <p>Numerical optimization is a fascinating field in its own which cannot be done justice in one article. There are many interesting aspects that we have not discussed, such as non-convex, non-smooth functions, as well as more sophisticated algorithms and the convergence properties of algorithms. In the following, I have included some references that I found useful, as well as references for further reading. Enjoy, and until next time!</p> <h2 id="references">References</h2> <ul> <li>Nocedal, J., &amp; Wright, S. (2006). <em>Numerical optimization.</em> Springer Science &amp; Business Media.</li> <li> <a href="https://en.wikipedia.org/wiki/Gradient_descent" rel="external nofollow noopener" target="_blank">en.wikipedia.org/wiki/Gradient_descent</a> (as opened 17.12.2021).</li> <li> <a href="https://en.wikipedia.org/wiki/Backtracking_line_search" rel="external nofollow noopener" target="_blank">en.wikipedia.org/wiki/Backtracking_line_search</a> (as opened 17.12.2021).</li> </ul> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Boris Bonev. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-Q6TLSVD591"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-Q6TLSVD591");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>